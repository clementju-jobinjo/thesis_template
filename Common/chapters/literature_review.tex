% !TEX root = ../main.tex

\chapter{Literature review}
\label{ch:literature_review}

Computer-aided cancer detection is a trending topic in deep learning. Nowadays, articles tend to focus more on segmentation than classification, but classification still got a lot of attention over the last decade. 

\section{Prostate/PROSTATEx}
PROSTATEx: section \ref{sec:PROSTATEx}.

Song et al. \cite{07} proposed a DCNN method to detect prostate cancer based on the SPIE-AAPM-NCI PROSTATEx Challenge dataset. This dataset is composed of multiparametric MRIs (T2W, DWI, ADC, DCE, PD, Ktrans) for a total of 204 training patients which were split into a training, validation and test set. Their data processing approach kept T2W, DWI and ADC grayscale images only. After resampling each image to the same resolution, T2W, DWI and ADC images were first cropped ($65x65$px patch) with the lesion in the center and stacked per patient, resulting in images containing three grayscale channels. Thanks to this method, the same lesion is visible in the same area over the three channels. This increases the probability of detecting a cancer by ensuring a good visibilty for each lesion, since the latter is not necesarily as visible with each parameter. Images were then normalized based on the Z-score per patient and per sequence (T2W, DWI, ADC), i.e. by subtracting the mean before dividing by the standard deviation. The training (undefined number of times), validation (undefined number of times) and test images (11x) were augmented using -20 to 20$^\circ$ rotations, horizontal flipping, vertical sliding of less than 2 pixels and stretching by a factor between 0.9 and 1.1. Most of these processing techniques were reproductible, apart from the manual lesion contouring and labelling performed by a radiologist. 
Their model is a modified version of the well-known VGG16 model, including the addition of $1x1$ convolutions and dropout layers after each max pooling layer, and the use of the ELU activation function. The evaluation method for each patient and finding made an average of the 11 predictions resulting from the 11-time augmentation of the test set. The best results were obtained by using DWI images with the highest b-value only, reaching an AUC of $0.944$ with a 95$\%$ confidence interval ($0.876$-$0.994$). However, this model was not tested on the official PROSTATEx challenge images, which is an interesting benchmark to evaluate how well a model generalizes.

Saifeng et al. \cite{31} created another architecture called XMasNet which was tested on the actual PROSTATEx challenge, achieving the second best performance at the time with an AUC of $0.84$. The AUC on their validation set reached $0.92$. Regarding data processing, their approach stacked different combinations of the available sequences as the three channels instead of defining a single combination like Song et al.: DWI-ADC-Ktrans, DWI-ADC-T2W, ADC-Ktrans-T2W and DWI-Ktrans-T2W. The data augmentation process differs in that the images are rotated in 3D, each lesion being sliced at 7 different orientations. These 2-dimensional slices were then augmented using rotation, shearing and translation of 1px, resulting in 207144 training samples. Both validation and testing test were also augmented in the same manner. This whole process allows to include 3-dimensional information in 2-dimensional images. The method used ensemble learning which combined different models the reach the best performance possible. 

Mehrtash et al. \cite{01} used a different approach. First of all, the input was feeded to three separated parts of the model, each one responsible for a specific sequence among ADC, maximum b-value DWI and Ktrans. Then, each of these feature extractors' outputs were merged into a common decision maker. Furthermore, 3-dimensional convolutions instead 2-dimensional ones were performed. In fact, 3-dimensional patches centered on the lesion were cropped. Augmentation including translation and flipping was used in order to balance the dataset. Apart from these differences, other minor differences such as normalizing the images within the range $[0,1]$ exist compared to the previous papers. These tricks allowed their model achieved an AUC of $0.80$ on the PROSTATEx challenge. To make predictions, five different models were used, averaging the predictions of the four best models. 


\section{Lung/Lung CT Challenge}
SPIE-AAPM Lung CT Challenge is lung cancer classification dataset. It is described in detail in section \ref{sec:lungCTChallenge}.

Cengil et al. \cite{02} built a fairly simple convolutional neural network to classify images of the Lung CT Challenge dataset. The model takes 4-dimensional data as input (depth, height, width and channels) and performs 3D convolutions on it. The model consists of an input layer, five layers of 3D convolutions (the first is associated with a RELU activation function and pooling, the last with nothing, and the others with pooling) and a fully connected layer at the end. Regarding the model evaluation, authors announce an accuracy of 0.7 on their test set, composed of 30 findings.


\section{Brain/Kaggle Brain}
Kaggle brain dataset: section \ref{sec:kaggleBrain}.

%Hanwat et al. \cite{32} implemented a convolutional neural network to classify brain tumors of the Kaggle dataset. Their processing method included skull masking, i.e. the removal of every non-brain tissue from the image. According to the authors, this technique improves performance quite a lot. ---> Check their methods (graphs look special)

Saxena et al. \cite{31} implemented three convolution neural networks to classify the brain tumors coming from the Kaggle "Brain MRI Images for Brain Tumor Detection" dataset. Their processing method used a cropping technique which removed extra black margin around the skull. Each border of the image merges with a part of the skull. Since the images come from different sources, their resolution vary quite a lot. Therefore, the authors resized them to $224x224x3$. Moreover, data was augmented by rotation and vertical/horizontal shifting. As the cropping was performed before augmenting the images, small parts of the brain are outside of the augmented images due to rotation and shifting. The data was split into a training set, a validation set and a test set. Regarding the models, authors implemented three of them (a Resnet-50, a VGG-16 and an Inception-V3) in order to compare their performance. The best results on the test set were achieved by the Resnet-50 (AUC of $0.95$ and accuracy of $0.95$). The VGG-16 was close (AUC of $0.90$ and accuracy of $0.90$), whereas the Inception-V3 didn't perform well (AUC of $0.55$ and accuracy of $0.55$).