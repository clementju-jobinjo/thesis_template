% !TEX root = ../main.tex

\chapter{Introduction}
\label{ch:introduction}

According to the World Health Organization, cancer caused $9.6$ million deaths in 2018, making it the second leading cause of death~\cite{44}. An early detection and treatment increases the chances of recovery. In this context, Computer-Aided Diagnosis (CADx) systems can play a massive role by preventing health professionals from missing positive diagnoses. Thanks to the attention that deep learning has gotten over the last decade, newer and better diagnosis systems have transfered the advantages of deep learning to cancer detection, diagnosis and localization tasks.

Deep learning applications require a lot of data to perform well. While certain fields profit from massive amounts of publicly available data, the medical field is quite the opposite. First of all, medical information is protected by the doctor-patient confidentiality and cannot be shared freely. As a consequence, data first has to be collected, organized and anonymized. Then, additional information regarding the clinical significance of the samples, the location of the lesions, etc. must be provided so that deep learning models can make use of the data. This whole process is time consuming and medical institutions don't always see the benefits which could ensue from publishing datasets of good quality. As a consequence, the lack of data is one of the toughest challenge related to this field. In order to overcome it, techniques such as transfer learning exist. The latter aims at using independent but similar datasets in order to increase the performance of a model on a target dataset. In other words, models are trained on the former in order to learn relevant features which improve the results on the latter. In the case of cancer detection and classification, only few datasets are available for each body part. Therefore, the idea behind this work is to make use of datasets of different body parts to classify one specific type of cancer.

This work is divided into various parts. First, the related literature was reviewed in order to gather techniques and architectures which gave decent results.

Second, deep learning is presented from the ground up under the historical and technical points of view. This section demystifies the topic by making an overview of the mathematical concepts and definitions related to it, which makes the understanding of the technical part possible without any background in the field.

The next chapter deals with medical knowledge related to cancer. Characteristics of the disease are presented, before focusing on the various file formats which medical imaging data are exported into. The last part is critical since a lot of data processing was performed in order to be able to make use of these files.

Then, the architecture presented in a prostate cancer classification paper was reproduced. This chapter allows to set a baseline proving that the methods and implementations work, from the image preprocessing to the neural network.

Finally, the last part is dedicated to transfer learning and its application to cancer classification.


% The most lethal kind is the lung cancer ($1.76$ million deaths in 2018).

\section{Motivation}
xx


\section{Contributions}
This work proposes various methods to detect cancer on MRIs, with a focus on prostate cancer. Concretely, the major contributions of this work are:
\begin{itemize}
\item Processing scripts for the PROSTATEx dataset, the Lung CT Challenge dataset and the Kaggle Brain dataset. This includes the conversion of DICOM/.png files to NumPy arrays, their registration (alignment for stacking and same resolution to have the same amount of tissues on each ) and their split into multiple sets to be given as input to machine learning algorithms.
\item Visualization scripts for DICOM, NIFTI and RAW medical file formats. A system to navigate through 4D data (width, height, depth and time) using the directional arrows is implemented: left/right arrows to vary the time axis and up/down to navigate through the difference slices (i.e the depth) of all images of a patient.
\item A state of the art deep learning model for prostate cancer detection. All steps are clearly described, the corresponding scripts are available on Github and the unadulterated results are presented. This allows a complete reproducibility of the experiment.
\item Processing script for the PROSTATEx challenge. This goes from the processing of the test images of the challenge to the generation of the .csv file containing their output predictions, which is either "benign lesion" or "malignant tumor".
\item A transfer learning pipeline that makes use of different body parts to increase the classification performance of one chosen body part. This technique is a concrete solution to overcome the lack of available data for each body part. Furthermore, it increases the generalization ability of the neural network.
\item Verification scripts to check the gradient flow of a neural network, the cropping of images, the presence of NaN value in images etc. All these scripts can be easily be imported in other projects.




 
\end{itemize}