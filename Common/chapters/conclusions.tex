% !TEX root = ../main.tex

\chapter{Conclusion}
\label{ch:conclusions}

\section{Conclusion}
\setlength{\marginparwidth}{3cm}\leavevmode \marginnote{\textbf{Cl{\'e}ment}}This work presented the process leading to the development of a deep learning system to classify potentially cancerous lesions, as well as strategies to overcome field-related issues such as the lack of data.

The starting point was the reproduction of Song et al.'s experiment~\cite{07}. Reaching good performance in this part despite being able to reproduce every single trick showed that our processing and training methods worked well. This resulted in a solid baseline that was exploited in order to take part in the "SPIE-AAPM-NCI Prostate MR Classification Challenge‚Äù, also called PROSTATEx challenge. Various hyperparameters and ways of processing the data were tested in order to reach an AUC of $0.76$ on this challenge. This score would have placed the model at the 15$^{th}$ position out of 71 submissions at the time of the challenge~\cite{42}, which confirms the robustness of the latter.

Then, the work focused on overcoming one of the main issues in deep learning: the lack of data. To achieve this, transfer learning, as well as more common techniques such as data augmentation, were applied. Our transfer learning implementation alternated between frozen and unfrozen steps and made use of brain and lung datasets to increase the model performance on a prostate dataset. During frozen steps, the first part of the model (the feature extractor) did not update its weights at all, whereas the second part (the decision maker) did. Experiments showed that our method allowed to increase the AUC on our test set by approximately 18\%, from $0.68$ before transfer learning to $0.80$ after transfer learning. As a less-experienced radiologist can aim for an AUC of $0.81$~\cite{42}, some adjustments (better model architecture, other processing and augmentation techniques, more data, more time to optimize the hyperparameters, ...) could produce a trustworthy model capable of helping diagnose cancer.

Throughout this thesis, various reusable tools were developed: visualization of medical imaging files, conversion of medical imaging files to PNG, a PyTorch sampler using undersampling, easy-to-use processing scripts for multiple datasets (PROSTATEx, Kaggle Brain, Lung CT Challenge), processing verification tools (red dot images), training verification tools (gradient flow graphs, metrics plots using Tensorboard), all-in-one training and testing files which can be adapted to new models and datasets, an end-to-end transfer learning pipeline. All these elements can be used as a baseline for future works or as additions to existing projects.


\section{Future Work}
\setlength{\marginparwidth}{3cm}\leavevmode \marginnote{\textbf{Jobin \& Cl{\'e}ment}}This work mainly used a single model architecture based on a VGG-16 network. Trying other architectures with other kernel sizes and a different number of layers can lead to better performance. Moreover, as MRIs are three-dimensional, using three-dimensional convolutions could improve the results as MRIs are three-dimensional. This would also require new processing techniques to generate volumes instead of two-dimensional images. 

However, well-labeled classification datasets can be counted on the fingers of one hand. As deep learning models need large amounts of training data, the continuation of the same work is more or less compromised due to the lack of data. On the other hand, tumor segmentation has become popular over the last years. Currently, most cancer-related deep learning challenges aim at segmenting tumors in three-dimensional data, which makes such datasets easy to find. Therefore, the experience acquired in this work could be transposed to segmentation tasks, which could even combine segmentation and classification. 

Another possibility to deal with the small amount of publicly available datasets is to develop partnerships with hospitals in order to have access to their data. Obviously, this raises questions about data confidentiality and ethics regarding how the latter is used. New platforms ensuring their confidentiality, their public sharing and their quality have to be developed with the goal of encouraging hospitals to share them. A lot of work has to be done in this regard. This would allow to build even better models in an easier way and make research move forward.

