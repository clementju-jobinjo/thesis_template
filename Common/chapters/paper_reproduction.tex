% !TEX root = ../main.tex

\chapter{Paper reproduction}
\label{ch:paper_reproduction}
This chapter is based on the article "Computer-Aided Diagnosis of Prostate Cancer Using a Deep Convolutional Neural Network from Multiparametric MRI" from Song et al.\cite{07}, shortly presented in chapter \ref{ch:literature_review}. It aims at reproducing the experiment of the paper in order to acquire the medical, theoretical and technical background before proposing a transfer learning method as a way to improve the classification using other body parts, which overcome the lack of available data in the medical field (see chapter \ref{ch:transfer_learning}).\\
Song et al. \cite{07} proposed a deep convolutional neural network (DCNN) method to detect prostate cancer based on the SPIE-AAPM-NCI PROSTATEx Challenge dataset. This dataset is one of the biggest available dataset for prostate cancer classification. The two different output classes of the latter are benign lesion (class 0) and malignant lesion (class 1). This paper explains all the steps to follow with the goal of building a computer-aided-diagnosis (CAD) system for prostate cancer detection. Moreover, it also provides results about the algorithm performance, which can be used as a benchmark to compare with the results of the reproduction of the experiment. However, since the article gives no information about the results their model got on the official PROSTATEx challenge test set, the reproduction of the experiment will fill that gap.\\
This chapter is built top-down: first a superficial overview of the entire process is established in order to understand the purpose of the experiment as a whole. Then, each part or theoretical notion of the experiment is described in detail. The structure of the dataset is exposed, all steps of its processing are explained and the techniques used as verification of the proper functioning of the algorithm are presented. After the processing of the data, the training phase is dealt with in depth and all hyperparameters, options and implementation choices are described to ensure the reproductibility of the experiment. This part is followed by the presentation of the raw results, which is itself followed by their analysis in the "discussion" section.

\section{Process overview}
Schematically speaking, the entire experiment process can be represented as shown on figure \ref{fig:paper_reproduction_process}. The PROSTATEx dataset is composed of samples whose clinical significance is provided (labeled data) and samples without this information (unlabeled data). The latter are used for the challenge, which consists in predicting the probability of patients lesions to be malignant (class 1) or benign (class 0). In both cases, the data is processed before been used (see \ref{sec:prostatex_data_processing} for details). After the processing, the labeled data are split into training, validation and test sets using respectively 80\%, 10\% and 10\% of all available data for each set. The training set is then given in batches as input to the neural network, which update its weight consequently. At the end of each epoch, the current model is tested on the validation set (that the model has never seen previously) and the metrics are plotted on Tensorboard (see \ref{paper_tensorboard}). If the current model has a bigger AUC and a bigger accuracy than the previous best model, it becomes the best model and it is saved. When the training phase reaches the defined number of epochs, the model is tested on the test set. The same model is also used to generated predictions for each patient and finding of the PROSTATEx challenge dataset in order to take part in it. 

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth, keepaspectratio=true]{./figures/paper_reproduction_process.png}
\caption{Paper reproduction experiment}
\label{fig:paper_reproduction_process}
\end{figure}


\section{PROSTATEx: Data processing}
\label{sec:prostatex_data_processing}
\subsection{Dataset description}
\label{sec:prostatex_dataset_description}
The SPIE-AAPM-NCI PROSTATEx Challenge dataset is publicly available on the Cancer Imaging Archive. This dataset is composed of multiparametric MRIs (T2W, DWI, ADC, DCE, PD, Ktrans) for a total of 204 training patients and 208 challenge patients. Images were taken under the sagittal, transverse and coronal planes.

The PROSTATEx dataset comes with two CSV files for the training set. The first one, \textit{ProstateX-Findings-Train.csv}, lists all findings with their clinical significance. Multiple findings can be associated with the same patient (ProxID). The second one, \textit{ProstateX-Images-Train.csv}, gives information about where to find the right DICOM file for each patient and each finding. Important labels are "ProxID" (patient ID), "fid" (finding ID $\in [1,\infty]$), "ClinSig" (clinical significance, TRUE or FALSE), "DCMSerNumber" (digit before the dash in the folder name containing DICOM files), "ijk" (position of the lesion: slice number $k$ at coordinates $(i,j)$, $i,j,k \in [0,\infty]$) and "VoxelSpacing" (3-dimensional vector representing the correspondance between a pixel and the space it occupies in the real world). Both CSV files are complementary to each other. Regarding the challenge patients, two analog CSV files are provided. The only difference is the absence of clinical significance. Finally, images are in the DICOM file format (see section \ref{sec:DICOM}). 


\subsection{Methodology}
The methodology described in this section is the closest replication possible of the authors' data processing methodology. Most of it were reproductible in a similar way, apart from the manual lesion contouring which was performed by a qualified radiologist in the authors' case. Furthermore, the authors didn't mention any participation in the official PROSTATEx challenge. In order to get an unbiased evaluation of the performance of our model, these images were also processed as explained below in order to take part in the challenge, but were not augmented. Nevertheless, the authors created their own test set by splitting the PROSTATEx training images into a training set ($80\%$), a validation set ($10\%$) and a test set ($10\%$). They mentioned that the test set was augmented $11$ times but gave no information about the training and validation sets. Since the training set is imbalanced (3 false for 1 true), the true class was augmented more times than the false class ($60x$) in order to create balanced training and validation sets. 


\subsection{From DICOM to NumPy arrays}
\label{sec:DICOMtoNumPy}
Before anything else, T2W, DWI and ADC grayscale images were used, which means that DCE, PD and Ktrans images were left aside. Moreover, only images showing the prostate under the transverse plane were used. The reason for this is that the tumors are much more visible under this perspective. 
The first step consisted in converting DICOM files. Algorithm \ref{alg:PROSTATEx_preprocessing} describes the steps involved in converting PROSTATEx's DICOM files to NumPy arrays. The right slices were found thanks to the two CSV files. Important information such as the patient ID, the sequence, the lesion location and the voxel spacing were used as file output names, which allows to use these files independently for the next steps (i.e. without relying on the CSV files). 

\begin{algorithm}
    \caption{PROSTATEx preprocessing}
    \label{alg:PROSTATEx_preprocessing}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
        \Procedure{main}{$dataset\_folder, findings\_CSV, slices\_CSV, output\_folder$}
        		\State Create output directories: $"output\_folder/True", "output\_folder/False"$\\
        		\State $findings \gets read\_CSV(findings\_CSV)$ \Comment{ProstateX-Findings-Train.csv}
        		\State $slices \gets read\_CSV(slices\_CSV)$ \Comment{ProstateX-Images-Train.csv}
            \State $meta \gets merge(findings, slices)$\Comment{Both CSV files are complementary to each other.}\\
            \For{$row$ in $meta$}
            		\State $patient\_id \gets row["ProxID"]$
            		\State $finding\_id \gets row["fid"]$
            		\State $mri\_type\_number \gets row["DCMSerNumber"]$
            		\State $clinical\_significance \gets row["ClinSig"]$
            		\State $img\_i, img\_j, img\_k \gets row["ijk"]$
            		\State $slice\_number \gets img\_k + 1$ \Comment{CSV indexing in $[0,\infty]$, DICOM in $[1,\infty]$}\\
                \For{$visit\ in\ patient\_id$'s folder}
                		\For{$mri\_type$ in $visit$}
                			\If {$mri\_type$ starts with "$mri\_type\_number$-"}
                				\For{$dicom\_file$ in $mri\_type$}
                					\If {$slice\_number == dicom\_file.InstanceNumber$}
                						\State{$slice \gets normalize\_dicom(dicom\_file)$} \Comment{see section \ref{sec:dicom_data_manipulation}}
                						\State{Save $slice$ in $"output\_folder/clinical\_significance"$}
                					\EndIf
                				\EndFor
                			\EndIf
            			\EndFor
            		\EndFor
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm} 


\subsection{From NumPy arrays to augmented stacked images}
\label{sec:numpyToAugmentedStacked}
This step can be split into two substeps: aligning and stacking images before augmenting them. Once full images were converted to NumPy arrays, the images related to a specific lesion of a specific patient needed to be aligned and resampled to the same resolution. In fact, T2-weighted images have a much higher resolution than the DWI and ADC images (in this dataset at least). Without this operation, a single pixel on a DWI image would have covered a lot more tissue than a pixel of the corresponding T2-image. To perform the alignment, the lesion was localized on the three sequences than to the CSV file. Then, the goal was to crop a large patch which contained the same amount of tissue on the three images. This patch had to be centered on the lesion to ease the augmentation process. So, a fixed patch size was defined for the T2-image since its resolution was the highest. Then, using the voxel spacing information, the patch size required to cover the same amount of tissue was computed for the two other sequences (DWI, ADC). Finally, the three images were cropped, resized to the same resolution and stacked. Stacking images consists in putting each single image (grayscale) into a single array, as three different channels. At this point, the pixel $i,j$ of the three channels represents the exact same tissue, which means that the lesion is at the same position and is covered by the same number of pixels on each channel. This approach increases the probability of detecting a cancer by ensuring a good visibilty for each lesion, since the latter is not necesarily as visible on the three images.

An important part of this process is the normalization. In fact, images were normalized before beeing stacked, based on a Z-score, i.e. 

\begin{equation}
\label{eq:normalization}
	Pixel_{{i,j}_{normalized}} = \frac{Pixel_{i,j} - \mu}{\sigma}
\end{equation}

\noindent where $Pixel_{i,j} \in [0,255]$ is a grayscale pixel value, $\mu$ is the mean value of all images of the corresponding sequence for this patient and $\sigma$ the standard deviation of all images of the corresponding sequence for this patient. According to the authors, normalizing each sequence of each patient separately allows to keep slight contrast nuances which ultimately help the final diagnosis. In other words, all T2-weighted images belonging to a specific patient were concatenated into a single NumPy array. Then, the mean and standard devation of the array were computed. These values served as $\mu$ and $\sigma$ to normalize the T2-images of this specific patient. Same for this patient's DWI and ADC images. The same process was applied to all patients separately. 

Finally, the dataset was augmented in the same way as the authors. Concretely, images were rotated ($-20$ to $20^\circ$), flipped horizontally (probability of $0.5$) and slid horizontally (value $\in [-1, 0, 1]$ pixel). These techniques alone allowed to create a sufficient amount of data. Therefore, two other augmentation methods used by the authors (horizontal stretching by a factor $\in [0.9,1.1]$ and vertical sliding) were not made use of. Images were exported as NumPy arrays.


\subsection{From NumPy arrays to augmented non-stacked images}
Another way of processing and using the data was experimented with. Instead of stacking the images, each image was cropped and augmented separately. At the end, each image was given as separate input to the neural network. 


\subsection{Data processing verification}
\label{sec:dataProcessingVerification}

\subsubsection{Cropping verification using red dots}
In order to check the correctness of the augmentation process, images with a red dot at the lesion coordinates were generated. First of all, a red dot was placed on the full image thanks to the "i,j,k" attribute in the CSV file. This full image was exported. Then, images were augmented in the exact same manner as described in section \ref{sec:numpyToAugmentedStacked}. The expected result satisfies the following properties:
\begin{itemize}
	\item The red dot is localized in the center of the cropped and augmented image.
	\item The three cropped and augmented images within a stacked image contain the exact same tissue. 
	\item The three cropped and augmented images within a stacked image are not rotated/shifted/flipped differently. 
	\item A lesion is visible to the naked eye. 
\end{itemize}

\noindent Figure \ref{fig:reddot} shows an example generated during this test. While beeing exactly the same for the 3 images, the red dot looks bigger on the images whose resolution was smaller since it was drawn before resampling the images. These images come from the same exam performed on the same patient on the same day. 

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth, keepaspectratio=true]{./figures/test_red_dot.png}
\caption{Red dot test - Patient 0082, Finding ID 1\\From left to right: T2, DWI, ADC.}
\label{fig:reddot}
\end{figure}



\subsubsection{Alignment}
In addition to the previous test, an additional one aiming at checking the alignment of the images was created. This test ensures that the three images show the exact same content at the same spot over the three channels. 

\begin{figure}[!h]
\centering
\includegraphics[width=\textwidth, keepaspectratio=true]{./figures/alignment.png}
\caption{Alignment - Patient 0242 - Finding ID 1}
\label{fig:alignment}
\end{figure}


\section{Training the neural network}
\subsection{Architecture}
The model architecture used in the paper is a modified version of the VGG network from the Oxford's Visual Geometry Group (VGG). This model was initially designed as part of the Large Scale Visual Recognition Challenge 2014 that used the ImageNet dataset of 14 millions images belonging to 1000 classes. Figure \ref{fig:paper_model} illustrates its structure and its corresponding implementation in Python with the PyTorch framework.\\
It is first composed by three convolution-dropout-max-pooling blocks followed by three fully-connected-dropout blocks. Each convolutional box (in blue) in the figure represents in reality three layers: the convolutional layer, the batch normalization layer and the exponential linear unit (ELU) activation function. The same principle applies to the fully connected layer box (in orange) that is divided into a fully connected layer followed by the exponential linear unit. The last fully connected box (in purple) has the same structure except that the exponential linear unit is replaced by a softmax  function for classification.\\
In comparison to the original VGG, this model keeps the small filter size of 3x3 or 1x1, also doubles the number of filters after each convolution-dropout-max-pooling block and has a stride of 1 for all convolutions. It differs from the traditional VGG in that it makes use of a smaller number of layers since the task is simpler than the original one. Moreover, it uses exponential linear units instead of rectified linear units as activation functions and adds dropout and batch normalization layers.
\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth, keepaspectratio=true]{./figures/model_paper_manual.png}
\caption{Model architecture with the corresponding PyTorch code}
\label{fig:paper_model}
\end{figure}
\subsection{Tensorboard}
During the experiment the following metrics are computed: loss, accuracy, precision, recall, F1-score, specificity and AUC. These metrics are computed separately on the training and on the validation sets at the end of each epoch. They are then stored across all epochs and plotted on the same figure thanks to Tensorboard at the end of the process. Subsequently, this is also the case for the metrics measured on the test set.\\
"Tensorboard provides the visualization and tooling needed for machine learning experimentation:
\begin{itemize}
\item Tracking and visualizing metrics.
\item Visualizing the model graph (ops and layers).
\item Viewing histograms of weights, biases, or other tensors as they change over time.
\item Projecting embeddings to a lower dimensional space.
\item Displaying images, text, and audio data"\cite{39}.
\end{itemize}
In this experiment, Tensorboard is mainly used to plot the Matplotlib figures of the model performance. In addition to this, written reports regarding which model was the best and the results it achieved are also added to the dashboard.
\label{paper_tensorboard}


\subsection{Script options}
Since training a neural network requires datasets, hyperparameters and many more configuration choices, the creation of a generic script that accepts multiple options is necessary. In order to achieve this goal, the Python module "argparse" is extremely useful. The latter automatically generates help/usage messages and displays errors when the arguments type in by the user are invalid. Table \ref{fig:paper_reproduction_options} shows all options accepted by the script.


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{|llll|}
\hline
\rowcolor[HTML]{DAE8FC} 
\textbf{Command}            & \textbf{Description}                                                                                                                                                                                                & \textbf{Required}                                                           & \textbf{Type} \\ \hline
\textbf{--trainingSet1}     & Training set  path                                                                                                                                                                                                  & True                                                                        & String        \\ \hline
\textbf{--validationSet1}   & Validation set path                                                                                                                                                                                                 & True                                                                        & String        \\ \hline
\textbf{--batchsize}        & Number of samples per batch                                                                                                                                                                                         & True                                                                        & Int           \\ \hline
\textbf{--nbepochs}         & Number of epochs the training phase has to last                                                                                                                                                                     & True                                                                        & Int           \\ \hline
\textbf{--lr}               & Learning rate used in the optimizer                                                                                                                                                                                 & \begin{tabular}[c]{@{}l@{}}False\\ Default: 1e-3\end{tabular}               & Float         \\ \hline
\textbf{--lossfunction}     & \begin{tabular}[c]{@{}l@{}}Loss function name\\{[}CrossEntropyLoss, L1Loss, MSELoss{]}\end{tabular}                                                                                                            & \begin{tabular}[c]{@{}l@{}}False\\ Default: 'CrossEntropyLoss'\end{tabular} & String        \\ \hline
\textbf{--cuda-device}      & GPU name to run the experiment                                                                                                                                                                                      & \begin{tabular}[c]{@{}l@{}}False\\ Default: 'cuda'\end{tabular}             & String        \\ \hline
\textbf{--modeltoload}      & \begin{tabular}[c]{@{}l@{}}Pretrained model name \\If given, load it, otherwise randomly initialize it\end{tabular}                                                                                     & \begin{tabular}[c]{@{}l@{}}False\\ Default: ""\end{tabular}                 & String        \\ \hline
\textbf{--dropout}          & Dropout probability                                                                                                                                                                                                 & \begin{tabular}[c]{@{}l@{}}False\\ Default: 0.3\end{tabular}                & Float         \\ \hline
\textbf{--optimized-metric} & \begin{tabular}[c]{@{}l@{}}Metric to optimize\\ The best model during the training will be saved according to it\\{[}'auc', 'accuracy', 'precision', 'recall', 'f1score', 'specificity'{]}\end{tabular} & \begin{tabular}[c]{@{}l@{}}False\\ Default: 'auc'\end{tabular}              & String        \\ \hline
\textbf{--outputdirectory}  & Root of the output directory used by Tensorboard to save the models                                                                                                                                                 & True                                                                        & String        \\ \hline
\end{tabular}%
}
\caption{Complete list of script options}
\label{fig:paper_reproduction_options}
\end{table}


\subsection{Model roulette}
As stated by the authors, "the training process is sensitive to the parameter initialization" \cite{07}. Experience showed us that the exact same hyperparameters with two different initializations could give opposite results. Therefore, in order not to train the model vainly, a script which generates a given number of initializations was created. Each untrained model was then tested on the validation set since the best model during training is saved according to its performance on the validation set. The model whose score was the highest on a given metric was saved and used as base model for the experiments. 


\subsection{Experimental setup}
The best performing model was implemented with PyTorch using the Adam optimizer to update its weights, a learning rate of 1e-8 and the cross-entropy as loss function. The data was organized in batches of 128 samples and the experiment ran for 1300 epochs. The best model was selected with respect to the area under curve. Regarding the weight initialization, the best model from the roulette that maximized the F1-score was chosen as initial model.\\
The model was trained on an Nvidia GeForce GTX Titan X graphics card during approximately 15 hours.

\subsection{Training verification}
In deep learning, two main problems can occur during training. The first one is called the "vanishing gradient" problem and refers to the fact that it is possible for the loss function to compute extremely small gradients, near zero. Consequently, the weight update is also extremely small, which makes the neural network hard or impossible to train. The second problem, the "exploding gradient" is the opposite. Concretely, in this case, large gradients are computed, which results in huge weight updates that can even reach "NaN" values. This makes the model unstable and unable to learn from training data.  \\
As a result, it is important to analyze the gradient propagation across the network to ensure that it is not facing such problems. To achieve this goal, a visualization tool to display the gradients of the entire network was implemented. This allows to visually notice the gradient flow at a glance and to see its evolution during all epochs (one visualization per batch is generated).


\subsubsection{Gradient flow visualization}
The visualization displays the gradient flow through the entire neural network. It shows the name of each layer on the x-axis and its average gradient value on the y-axis (recall: for each layer the gradient is a matrix). Furthermore, it also gives information about the max value of the gradient and indicates in black if one of them is equal to zero.\\
The figure \ref{fig:gradient_flow} shows the gradient flow at epoch 0, batch 6. From the output layer (called "last\_layer0.weight" on the graph) to the first layer, the gradient is well propagated: no huge average gradients are registered, neither extremely small ones.

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth, keepaspectratio=true]{./figures/gradient_flow.png}
\caption{Gradient flow at epoch 0, batch 6 of the experiment}
\label{fig:gradient_flow}
\end{figure}
\section{Results}
Figures \ref{fig:paper_reprodution_results_part_1}, \ref{fig:paper_reprodution_results_part_2} and \ref{fig:paper_reprodution_results_part_3} show the different metrics registered on the training and validation sets during the training phase. The best model was chosen at epoch ... with an accuracy of ... and an AUC of ... (the condition for a model to be chosen as best is that the current AUC should be higher than the AUC of the previous best model, same for the accuracy).\\
This best model was also used to take part in the ProstateX challenge. The res XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth, height=\textheight, keepaspectratio=true]{./figures/paper_reproduction_results_part1.png}
\caption{Accuracy, AUC and loss on the training and validation set}
\label{fig:paper_reprodution_results_part_1}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth, height=\textheight, keepaspectratio=true]{./figures/paper_reproduction_results_part2.png}
\caption{F1-score, precision and recall on the training and validation set}
\label{fig:paper_reprodution_results_part_2}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[height=0.4\textheight, keepaspectratio=true]{./figures/paper_reproduction_results_part3.png}
\caption{Specificity on the training and validation set}
\label{fig:paper_reprodution_results_part_3}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[height=0.4\textheight, keepaspectratio=true]{./figures/paper_reproduction_results_challenge.png}
\caption{Score achieved in the PROSTATEx challenge}
\label{fig:paper_reprodution_results_challenge}
\end{figure}

\section{Discussion}
Thanks to all curves in the "results", a lot of information regarding the model performance can be inferred. In fact, "reviewing learning curves of models during training can be used to diagnose problems with learning, such as an underfit or overfit model, as well as whether the training and validation datasets are suitably representative"\cite{40}.\\
First of all, when observing the loss plot, what is noticeable is that the training and validation losses follow the same behaviour until the epoch 600 approximately. This is a clear sign of good fit of the model on the data. All features learned by the model on the training data are also applicable to the validation set. This situation is the optimal one. After this phase, the model slightly started to overfit. In fact, the training loss continues to linearly decrease whereas the validation loss decreases less quickly until reaching  XXXXXXXXXXXXXXXXXXXXXXXX\\

As the goal of the experiment was to reproduce the results of the paper, 

